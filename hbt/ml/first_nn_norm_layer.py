"""
First implementation of DNN for HH analysis, generalized (TODO)
"""

from __future__ import annotations

from typing import Any
import gc
import time
import os

import law
import order as od

from columnflow.ml import MLModel
from columnflow.util import maybe_import, dev_sandbox
from columnflow.columnar_util import Route, set_ak_column, remove_ak_column
from columnflow.tasks.selection import MergeSelectionStatsWrapper
from columnflow.tasks.production import ProduceColumns
from hbt.config.categories import add_categories_ml
from columnflow.columnar_util import EMPTY_FLOAT
from hbt.ml.plotting import (
    plot_loss, plot_accuracy, plot_confusion, plot_roc_ovr, plot_output_nodes, plot_significance,
    write_info_file, plot_shap_baseline, plot_feature_ranking_deep_sets, plot_shap_deep_sets,
    check_distribution, plot_shap_deep_sets_pp, plot_shap_deep_sets_ps, PCA,
)

np = maybe_import("numpy")
ak = maybe_import("awkward")
tf = maybe_import("tensorflow")
pickle = maybe_import("pickle")
keras = maybe_import("tensorflow.keras")
sklearn = maybe_import("sklearn")
it = maybe_import("itertools")
# dotted_dict = maybe_import("dotted_dict")

logger = law.logger.get_logger(__name__)

""" function creating pairs from all jets, returns the four vector sum of the pairs as e, mass,
pt, eta, phi, as well as delta eta/phi, adresses columns by number so
DON'T CHANGE ORDER IN pairs_input
currently only generates pairs, might sacel to higher comb sizes later
The mask used in generated byased of of e, but is applied to all, since all entries in the arrays
are part of the new pairs, therefore all arrays have the same amount of proper and padded
values/pairs per event -> mask is applicable to all arrays. The mask is used to fill the padded
pairs with the masking value, the propermasking is done in the NN through the masking value """
def create_pairs(jets, pairs_dict, pais_input, masking_val, max_jets):
    n_jets = (np.sum((jets != EMPTY_FLOAT), axis=1) / len(pais_input)).astype('i')
    pairs_idx = list(map(pairs_dict.get, n_jets.tolist()))
    jets_shaped = jets.reshape((len(n_jets), -1, len(pais_input)))
    max_pairs = int(np.math.factorial(max_jets) / (2 * np.math.factorial(max_jets - 2)))
    pairs = np.full((len(n_jets), max_pairs, 2, len(pais_input)), -1.)
    for i in range(len(n_jets)):
        pairs[i] = jets_shaped[i][pairs_idx[i]]
    four_vectors = np.sum(pairs[:, :, :, :4], axis=2)
    e = four_vectors[:, :, 0]
    mask_pairs = 2 * EMPTY_FLOAT # generate mask based on e that is applied to all,
    mask = (e != mask_pairs) # since all pairs (num of pairs per event equal for all)
    pt = np.sqrt(four_vectors[:, :, 1]**2 + four_vectors[:, :, 2]**2)
    sorting = np.argsort(np.where(mask, pt, masking_val), axis=1)
    sorting = np.fliplr(sorting)
    phi = np.arctan2(four_vectors[:, :, 2], four_vectors[:, :, 1])
    squared_p = four_vectors[:, :, 1]**2 + four_vectors[:, :, 2]**2 + four_vectors[:, :, 3]**2
    mass = np.nan_to_num(np.sqrt(four_vectors[:, :, 0]**2 - squared_p))
    y = 0.5 * np.log((e + four_vectors[:, :, 3]) / (e - four_vectors[:, :, 3]))
    delta_eta = abs(pairs[:, :, 0, 4] - pairs[:, :, 1, 4])
    delta_phi = abs(pairs[:, :, 0, 5] - pairs[:, :, 1, 5])
    btags = pairs[:, :, :, 6:]
    sum_btags = np.sum(btags, axis=2)
    stacking = np.array([e, mass, pt, y, phi, delta_eta, delta_phi, sum_btags[:, :, 0],
                         sum_btags[:, :, 1], sum_btags[:, :, 2], sum_btags[:, :, 3],
                         sum_btags[:, :, 4], sum_btags[:, :, 5], sum_btags[:, :, 6]])
    features_pairs = np.array(["Energy", "Mass", r"$p_{T}$", "y", r"$\phi$", r"$\Delta \eta$",
                              r"$\Delta \phi$", "Deep B", "Deep CvL", "Deep CvB", "Deep Flav B",
                               "Deep Flav CvL", "Deep Flav CvB", "Deep Flav QG"])
    mask_tiled = np.array([mask] * len(stacking))
    stacking_masked = np.where(mask_tiled, stacking, masking_val)
    pairs_inp = np.stack(stacking_masked, axis=-1)
    events_idx = np.arange(pairs_inp.shape[0])
    events_idx = events_idx.reshape(len(events_idx), -1)
    # sort the pairs for highest pt
    pairs_inp_sorted = pairs_inp[events_idx, sorting]
    # pairs_inp = tf.convert_to_tensor(pairs_inp)

    return pairs_inp, pairs_inp_sorted, features_pairs


def resort_jets(jet_input_shaped, jet_input_features, sorting_feature, masking_val):
    # takes as input the shaped jet input returned by reshape_norm_inputs
    jets = np.where(jet_input_shaped == masking_val, EMPTY_FLOAT, jet_input_shaped)
    sorting_idx = jet_input_features.index(sorting_feature)
    # apply argsort on the negative array to get a sorting for a descending order
    sorting_indices = np.argsort(-jets[:, :, sorting_idx])
    jets_new = np.full_like(jets, 0.0)
    for i in np.arange(jets.shape[0]):
        sorted_event = jets[np.arange(jets.shape[0])[i], sorting_indices[i]]
        jets_new[i] = sorted_event
    jets_new = np.where(jets == EMPTY_FLOAT, masking_val, jets_new)

    return jets_new


# dedicated function to get the phi array of the leading jet
def get_leading_phi_array(events, projection_phi):
    phi_str = 'phi'
    for fields_str in events.fields:
        if 'phi' in fields_str:
            phi_str = fields_str
            break
    leading_phi_array = getattr(events, phi_str)[:, 0]
    return leading_phi_array


# function that projects all phi values relative to the phi value of the leading jet
# leading jet is set to phi 0 and all other jet phi values are replaced by the distance delta phi to leading jet
def phi_projection(projection_phi_array, phi_array):
    projected_array = phi_array - projection_phi_array
    projected_array = ak.where(projected_array < -np.pi, projected_array + 2 * np.pi, projected_array)
    projected_array = ak.where(projected_array > np.pi, projected_array - 2 * np.pi, projected_array)
    # check that the values of phi are not empty float/padded jets, such that empty floats are not modified
    projected_array = ak.where(phi_array == EMPTY_FLOAT, phi_array, projected_array)
    return projected_array


# Define functions to normalize and shape inputs1 and 2
def reshape_raw_inputs1(events, n_features, input_features, projection_phi):
    # leading_phi = get_leading_phi_array(events)
    column_counter = 0
    num_events, max_jets = ak.to_numpy(events[input_features[0]]).shape
    zeros = np.zeros((num_events, max_jets * n_features))
    for i in range(max_jets):
        for jet_features in input_features:
            if 'phi' in jet_features:
                zeros[:, column_counter] = phi_projection(projection_phi, events[jet_features][:, i])
            else:
                zeros[:, column_counter] = events[jet_features][:, i]
            column_counter += 1
    return zeros


def reshape_raw_inputs2(events):
    events = ak.to_numpy(events)
    events = events.astype(
        [(name, np.float32) for name in events.dtype.names], copy=False,
    ).view(np.float32).reshape((-1, len(events.dtype)))

    if np.any(~np.isfinite(events)) or np.any(~np.isfinite(events)):
        raise Exception("Infinite values found in inputs from dataset.")

    return events


# returns a dict containg the correctly shaped inputs as well as mean and std for all inputs
def reshape_norm_inputs(events_dict, norm_features, input_features, n_op_nodes, baseline_jets, baseline_pairs, masking_val):
    """ reshape train['inputs'] for DeepSets: [#events, #jets, -1] and
    calculate mean and std for normalization in the NN. For the comutation of mean and std replace
    EMPTY_FLOATs with np.nan and use np.nanmean/nanstd instead. This avoids for loops"""
    n_features = len(input_features[0])
    tf_dtype = tf.float32

    inputs1 = events_dict["inputs"]
    events_shaped = inputs1.reshape((-1, n_features))
    mask_norm_features = np.isin(input_features[0], norm_features[0])
    jets_masked = np.where(events_shaped == EMPTY_FLOAT, np.nan, events_shaped)
    mean_jets = np.where(mask_norm_features, np.nanmean(jets_masked, axis=0), 0.)
    std_jets = np.where(mask_norm_features, np.nanstd(jets_masked, axis=0), 1.)
    jets_shaped = inputs1.reshape((len(inputs1), -1, n_features))
    jets_shaped = np.where(jets_shaped == EMPTY_FLOAT, masking_val, jets_shaped)
    events_dict["inputs"] = tf.convert_to_tensor(jets_shaped, dtype=tf_dtype)

    """ get mean and std for event level features and replace EMPTY FLOATS with 3sig or -1 padding.
    Padded versions of the mean/std_inputs2 arrays neew to be created because the FF network
    sees the already concatenated input of deepsets and event level features. Therefore, for all
    deep sets outputs use mean=0 and std=1 so that they remain unchanged."""
    inputs2 = events_dict["inputs2"]
    inputs2_masked = np.where(inputs2 == EMPTY_FLOAT, np.nan, inputs2)
    mask_norm_features2 = np.isin(input_features[1], norm_features[1])
    mean_inputs2 = np.where(mask_norm_features2, np.nanmean(inputs2_masked, axis=0), 0.)
    std_inputs2 = np.where(mask_norm_features2, np.nanstd(inputs2_masked, axis=0), 1.)
    inputs2 = np.where(inputs2 == EMPTY_FLOAT, 0, inputs2)
    events_dict["inputs2"] = tf.convert_to_tensor(inputs2, dtype=tf_dtype)

    """ prepare the input for the baseline comaprison network, taking jets and
    event level features as input. The mean and std are generated through the mean and std
    arrays already computed for inputs and inputs2. All Jets should be normalized using the
    same mean and std for each Jet, therefore repeat/tile the jet means and stds for the
    amount of jets (baseline_jets) included in the baseline input and concatenate the
    inputs2 mean and std array. """
    slicing_idx = len(input_features[0]) * baseline_jets
    jets_baseline = inputs1[:, :slicing_idx]
    jets_baseline = np.where(jets_baseline == EMPTY_FLOAT, masking_val, jets_baseline)
    baseline_inp = np.concatenate((jets_baseline, inputs2), axis=1)
    mean_baseline = np.concatenate((np.tile(mean_jets, baseline_jets), mean_inputs2))
    std_baseline = np.concatenate((np.tile(std_jets, baseline_jets), std_inputs2))
    events_dict["inputs_baseline"] = tf.convert_to_tensor(baseline_inp, dtype=tf_dtype)

    """ reshape of target """
    events_dict["target"] = tf.reshape(events_dict["target"], [-1, n_op_nodes])

    """ turn pairs inp into tf tensor and get the mean and std for normalization layer, the
    shape of pairs inp is already correct. The masking value was already set in create_pairs(),
    therefore there are no EMPTY_FLOATs for pair padding but the maskig_val instead"""
    pairs_inp = events_dict["pairs_inp"]
    pairs_inp = pairs_inp.reshape(-1, pairs_inp.shape[2])
    pairs_inp_masked = np.where(pairs_inp == masking_val, np.nan, pairs_inp)
    mean_pairs = np.nanmean(pairs_inp_masked, axis=0)
    std_pairs = np.nanstd(pairs_inp_masked, axis=0)
    # preserve original order for the concatenation of the pairs inp and the DS output
    events_dict["pairs_inp"] = tf.convert_to_tensor(events_dict["pairs_inp"], dtype=tf_dtype)

    """prepare the inputs for the baseline pairs NN. Reuse mean and std of jets and inputs2."""
    # seperate sorted pairs for the baseline pairs nn
    baseline_inp_pairs = events_dict["pairs_inp_sorted"][:, :baseline_pairs, :]
    baseline_inp_pairs = np.reshape(baseline_inp_pairs, (baseline_inp_pairs.shape[0], -1))
    baseline_inp_pairs = np.concatenate((jets_baseline, baseline_inp_pairs, inputs2), axis=1)
    mean_baseline_pairs = np.concatenate((np.tile(mean_jets, baseline_jets), np.tile(mean_pairs, baseline_pairs), mean_inputs2))
    std_baseline_pairs = np.concatenate((np.tile(std_jets, baseline_jets), np.tile(std_pairs, baseline_pairs), std_inputs2))
    events_dict["inputs_baseline_pairs"] = tf.convert_to_tensor(baseline_inp_pairs, dtype=tf_dtype)

    """" Save the mean and std arrays in seperate dict and later use only the means and stds
    dict that was based of of the train input, pass that to the NN for the normalization layer"""
    norm_dict = {}
    norm_dict["mean_jets"] = tf.convert_to_tensor(mean_jets, dtype=tf_dtype)
    norm_dict["std_jets"] = tf.convert_to_tensor(std_jets, dtype=tf_dtype)
    norm_dict["mean_inputs2"] = tf.convert_to_tensor(mean_inputs2, dtype=tf_dtype)
    norm_dict["std_inputs2"] = tf.convert_to_tensor(std_inputs2, dtype=tf_dtype)
    norm_dict["mean_baseline"] = tf.convert_to_tensor(mean_baseline, dtype=tf_dtype)
    norm_dict["std_baseline"] = tf.convert_to_tensor(std_baseline, dtype=tf_dtype)
    norm_dict["mean_pairs"] = tf.convert_to_tensor(mean_pairs, dtype=tf_dtype)
    norm_dict["std_pairs"] = tf.convert_to_tensor(std_pairs, dtype=tf_dtype)
    norm_dict["mean_baseline_pairs"] = tf.convert_to_tensor(mean_baseline_pairs, dtype=tf_dtype)
    norm_dict["std_baseline_pairs"] = tf.convert_to_tensor(std_baseline_pairs, dtype=tf_dtype)

    return events_dict, norm_dict


class SimpleDNN(MLModel):

    def __init__(
            self,
            *args,
            folds: int | None = None,
            n_features: int | None = None,
            ml_process_weights: dict | None = None,
            model_name: str | None = None,
            n_output_nodes: int | None = None,
            activation_func_deepSets: str | None = None,
            activation_func_ff: str | None = None,
            batch_norm_deepSets: bool | None = None,
            batch_norm_ff: bool | None = None,
            nodes_deepSets: list | None = None,
            nodes_ff: list | None = None,
            aggregations: list | None = None,
            norm_features: list | None = None,
            empty_overwrite: str | None = None,
            quantity_weighting: bool | None = None,
            jet_num_cut: int | None = None,
            baseline_jets: int | None = None,
            model_type: str | None = None,
            **kwargs,
    ):
        """
        Parameters that need to be set by derived model:
        folds, layers, learningrate, batchsize, epochs, eqweight, dropout,
        processes, ml_process_weights, dataset_names, input_features, store_name,
        """

        single_config = True  # noqa

        super().__init__(*args, **kwargs)

        # class- to instance-level attributes
        # (before being set, self.folds refers to a class-level attribute)
        self.folds = folds or self.folds
        self.n_features = n_features or self.n_features
        self.ml_process_weights = ml_process_weights or self.ml_process_weights
        self.model_name = model_name or self.model_name
        self.n_output_nodes = n_output_nodes or self.n_output_nodes
        self.aggregations = aggregations or self.aggregations
        self.batch_norm_ff = batch_norm_ff or self.batch_norm_ff
        self.batch_norm_deepSets = batch_norm_deepSets or self.batch_norm_deepSets
        self.activation_func_deepSets = activation_func_deepSets or self.activation_func_deepSets
        self.activation_func_ff = activation_func_ff or self.activation_func_ff
        self.nodes_deepSets = nodes_deepSets or self.nodes_deepSets
        self.nodes_ff = nodes_ff or self.nodes_ff
        self.norm_features = norm_features or self.norm_features
        self.quantity_weighting = quantity_weighting or self.quantity_weighting
        self.jet_num_cut = jet_num_cut or self.jet_num_cut
        self.baseline_jets = baseline_jets or self.baseline_jets
        self.model_type = model_type or self.model_type

    def setup(self):
        # dynamically add variables for the quantities produced by this model
        for proc in self.processes:
            if f"{self.cls_name}.score_{proc}" not in self.config_inst.variables:
                self.config_inst.add_variable(
                    name=f"{self.cls_name}.score_{proc}",
                    null_value=-1,
                    binning=(40, 0., 1.),
                    # x_title=f"DNN output score {self.config_inst.get_process(proc).x.ml_label}",
                )
                hh_bins = [0.0, .4, .45, .5, .55, .6, .65, .7, .75, .8, .85, .92, 1.0]
                bkg_bins = [0.0, 0.4, 0.7, 1.0]
                self.config_inst.add_variable(
                    name=f"{self.cls_name}.score_{proc}_rebin1",
                    expression=f"{self.cls_name}.score_{proc}",
                    null_value=-1,
                    binning=hh_bins if "HH" in proc else bkg_bins,
                    # x_title=f"DNN output score {self.config_inst.get_process(proc).x.ml_label}",
                )

        # one variable to bookkeep truth labels
        # TODO: still needs implementation
        if f"{self.cls_name}.ml_label" not in self.config_inst.variables:
            self.config_inst.add_variable(
                name=f"{self.cls_name}.ml_label",
                null_value=-1,
                binning=(len(self.processes) + 1, -1.5, len(self.processes) -0.5),
                x_title=f"DNN truth score",
            )

        # dynamically add ml categories (but only if production categories have been added)
        if (
                self.config_inst.x("add_categories_ml", True) and
                not self.config_inst.x("add_categories_production", True)
        ):
            add_categories_ml(self.config_inst, ml_model_inst=self)
            self.config_inst.x.add_categories_ml = False

    def requires(self, task: law.Task) -> str:
        # add selection stats to requires; NOTE: not really used at the moment
        all_reqs = MergeSelectionStatsWrapper.req(
            task,
            shifts="nominal",
            configs=self.config_inst.name,
            datasets=self.dataset_names,
        )

        return all_reqs

    def sandbox(self, task: law.Task) -> str:
        return dev_sandbox("bash::$CF_BASE/sandboxes/venv_ml_tf_dev.sh")

    def datasets(self, config_inst: od.Config) -> set[od.Dataset]:
        return {config_inst.get_dataset(dataset_name) for dataset_name in self.dataset_names}

    def uses(self, config_inst: od.Config) -> set[Route | str]:
        return ({"normalization_weight", "category_ids"} |
                set(self.input_features[0]) |
                set(self.input_features[1]) |
                set(self.projection_phi) |
                {"CustomVBFMaskJets2_px", "CustomVBFMaskJets2_py", "CustomVBFMaskJets2_pz"})

    def produces(self, config_inst: od.Config) -> set[Route | str]:
        produced = set()
        for proc in self.processes:
            produced.add(f"{self.cls_name}.score_{proc}__{self.model_name}")
            produced.add(f"{self.cls_name}.predictions_{proc}__{self.model_name}")
            produced.add(f"{self.cls_name}.ml_truth_label_{proc}__{self.model_name}")
            produced.add(f"{self.cls_name}.pred_target_{proc}__{self.model_name}")
            produced.add(f"{self.cls_name}.target_label_{proc}__{self.model_name}")
            produced.add(f"{self.cls_name}.events_weights_{proc}__{self.model_name}")
            produced.add(f"mlscore.{proc}")
            for i in range(self.folds):
                produced.add(f"{self.cls_name}.pred_fold_{i}_{proc}__{self.model_name}")
                produced.add(f"{self.cls_name}.pred_model_{proc}_fold{i}__{self.model_name}")
                produced.add(f"{self.cls_name}.DeepSetsInpPt_{proc}_fold{i}__{self.model_name}")
                produced.add(f"{self.cls_name}.DeepSetsInpEta_{proc}_fold{i}__{self.model_name}")

        produced.add("category_ids")

        return produced

    def output(self, task: law.Task) -> law.FileSystemDirectoryTarget:
        return task.target(f"mlmodel_f{task.branch}of{self.folds}_{self.model_name}", dir=True)

    def open_model(self, target: law.LocalDirectoryTarget) -> tf.keras.models.Model:
        # return target.load(formatter="keras_model")

        # with open(f"{target.path}/model_history.pkl", "rb") as f:
        #     history = pickle.load(f)
        model = tf.keras.models.load_model(target.path)
        return model, None

    def training_configs(self, requested_configs: Sequence[str]) -> list[str]:
        # default config
        # print(requested_configs)
        if len(requested_configs) == 1:
            return list(requested_configs)
        else:
            # TODO: change to "config_2017" when finished with testing phase
            return ["config_2017_limited"]

    def training_calibrators(self, config_inst: od.Config, requested_calibrators: Sequence[str]) -> list[str]:
        # fix MLTraining Phase Space
        return ["skip_jecunc"]

    def evaluation_calibrators(self, config_inst: od.Config, requested_calibrators: Sequence[str]) -> list[str]:
        # fix MLTraining Phase Space
        return ["skip_jecunc"]

    def training_selector(self, config_inst: od.Config, requested_selector: str) -> str:
        # fix MLTraining Phase Space
        return "default"

    def training_producers(self, config_inst: od.Config, requested_producers: Sequence[str]) -> list[str]:
        # fix MLTraining Phase Space
        return ["default"]

    def prepare_inputs(
        self,
        task,
        input,
    ) -> dict[str, np.array]:
        # set seed for shuffeling for reproducebility
        np.random.seed(1337)
        tf.random.set_seed(1337)

        self.features_pairs = str()
        self.pairs_dict_tf = {}
        for k, v in self.pairs_dict_pad.items():
            self.pairs_dict_tf[k] = tf.convert_to_tensor(v)
        '''Kepp the -1 as a dummy value for the padded pairs, based on the -1 the mask in the
        CreatePairs layer is computed to mask the pairs.'''
        self.ml_process_factors = dict(self.ml_process_weights)
        self.ml_process_weights.update(dict.fromkeys(self.ml_process_weights.keys(), 0))
        if self.quantity_weighting:
            # properly define the ml_process_weights through the number of events per dataset
            for dataset0, files0 in input["events"][self.config_inst.name].items():
                t0 = time.time()

                dataset_inst = self.config_inst.get_dataset(dataset0)
                if len(dataset_inst.processes) != 1:
                    raise Exception("only 1 process inst is expected for each dataset")

                N_events = sum([len(ak.from_parquet(inp["mlevents"].fn)) for inp in files0])
                process_name = "_".join(dataset0.split("_")[:-1])
                process_name = "tt" if "tt" in process_name and "tt" in self.processes else process_name
                process_name = "dy" if "dy" in process_name and "dy" in self.processes else process_name
                self.ml_process_weights[process_name] += N_events
            max_events_of_proc = np.max(list(self.ml_process_weights.values()))
            for key_proc, value_proc in self.ml_process_weights.items():
                self.ml_process_weights[key_proc] = max_events_of_proc / value_proc
                self.ml_process_weights[key_proc] *= self.ml_process_factors[key_proc]

        # max_events_per_fold = int(self.max_events / (self.folds - 1))
        self.process_insts = []
        # self.input_features[0].remove(self.projection_phi[0])
        for i, proc in enumerate(self.processes):
            proc_inst = self.config_insts[0].get_process(proc)
            proc_inst.x.ml_id = i
            proc_inst.x.ml_process_weight = self.ml_process_weights.get(proc, 1)

            self.process_insts.append(proc_inst)

        process_insts = [self.config_inst.get_process(proc) for proc in self.processes]
        N_events_processes = np.array(len(self.processes) * [0])
        ml_process_weights = np.array(len(self.processes) * [0])
        sum_eventweights_processes = np.array(len(self.processes) * [0])
        dataset_proc_idx = {}  # bookkeeping which process each dataset belongs to

        #
        # determine process of each dataset and count number of events & sum of eventweights for this process
        #
        for dataset, files in input["events"][self.config_inst.name].items():
            t0 = time.time()

            dataset_inst = self.config_inst.get_dataset(dataset)
            if len(dataset_inst.processes) != 1:
                raise Exception("only 1 process inst is expected for each dataset")

            # TODO: use stats here instead
            N_events = sum([len(ak.from_parquet(inp["mlevents"].fn)) for inp in files])
            # NOTE: this only works as long as each dataset only contains one process
            sum_eventweights = sum([
                ak.sum(ak.from_parquet(inp["mlevents"].fn).normalization_weight)
                for inp in files],
            )
            for i, proc in enumerate(process_insts):
                ml_process_weights[i] = self.ml_process_weights[proc.name]
                leaf_procs = [p for p, _, _ in self.config_inst.get_process(proc).walk_processes(include_self=True)]
                if dataset_inst.processes.get_first() in leaf_procs:
                    logger.info(f"the dataset *{dataset}* is used for training the *{proc.name}* output node")
                    dataset_proc_idx[dataset] = i
                    N_events_processes[i] += N_events
                    sum_eventweights_processes[i] += sum_eventweights
                    continue

            if dataset_proc_idx.get(dataset, -1) == -1:
                raise Exception(f"dataset {dataset} is not matched to any of the given processes")

            logger.info(f"Weights done for {dataset} in {(time.time() - t0):.3f}s")

        # Number to scale weights such that the largest weights are at the order of 1
        # (only implemented for eqweight = True)
        weights_scaler = min(N_events_processes / ml_process_weights)

        #
        # set inputs, weights and targets for each datset and fold
        #
        DNN_inputs = {
            "weights": None,
            "inputs": None,
            "inputs2": None,
            "target": None,
        }

        sum_nnweights_processes = {}

        self.target_dict = {}

        for dataset, files in input["events"][self.config_inst.name].items():
            t0 = time.time()
            this_proc_idx = dataset_proc_idx[dataset]
            proc_name = self.processes[this_proc_idx]
            N_events_proc = N_events_processes[this_proc_idx]
            sum_eventweights_proc = sum_eventweights_processes[this_proc_idx]

            logger.info(
                f"dataset: {dataset}, \n  #Events: {N_events_proc}, "
                f"\n  Sum Eventweights: {sum_eventweights_proc}",
            )
            sum_nnweights = 0

            self.target_dict[f'{proc_name}'] = this_proc_idx
            for inp in files:
                events = ak.from_parquet(inp["mlevents"].path)
                weights = events.normalization_weight
                if self.eqweight:
                    weights = weights * weights_scaler / sum_eventweights_proc
                    custom_procweight = self.ml_process_weights[proc_name]
                    weights = weights * custom_procweight

                weights = ak.to_numpy(weights)

                if np.any(~np.isfinite(weights)):
                    raise Exception(f"Infinite values found in weights from dataset {dataset}")

                sum_nnweights += sum(weights)
                sum_nnweights_processes.setdefault(proc_name, 0)
                sum_nnweights_processes[proc_name] += sum(weights)
                # remove columns not used in training
                projection_phi = events[self.projection_phi[0]]
                events_pairs = events[self.pair_vectors]
                for var in events.fields:
                    if var not in self.input_features[0] and var not in self.input_features[1]:
                        print(f"removing column {var}")
                        events = remove_ak_column(events, var)

                # make a cut on events based on a min number of jets required per event
                # get the string name of njets for the given jet collection
                njets_field = [i for i in self.input_features[1] if 'njets' in i][0]
                events_n_jets = getattr(events, njets_field)
                mask = events_n_jets > self.jet_num_cut
                weights = weights[mask]
                events_new = {}
                for feature in events.fields:
                    events_new[feature] = events[feature][mask]
                events_new = ak.Array(events_new)

                events2 = events_new[self.input_features[1]]
                events = events_new[self.input_features[0]]

                # reshape raw inputs
                events = reshape_raw_inputs1(events, self.n_features, self.input_features[0], projection_phi)
                events_pairs = reshape_raw_inputs1(events_pairs, len(self.pair_vectors), self.pair_vectors, projection_phi)
                pairs_inp, pairs_inp_sorted, features_pairs = create_pairs(events_pairs, self.pairs_dict_pad, self.pair_vectors, self.masking_val, 10)
                self.features_pairs = features_pairs
                events2 = reshape_raw_inputs2(events2)

                # create the truth values for the output layer
                target = np.zeros((len(events), len(self.processes)))
                target[:, this_proc_idx] = 1
                if np.any(~np.isfinite(target)):
                    raise Exception(f"Infinite values found in target from dataset {dataset}")
                if DNN_inputs["weights"] is None:
                    DNN_inputs["weights"] = weights
                    DNN_inputs["inputs"] = events
                    DNN_inputs["inputs2"] = events2
                    DNN_inputs["pairs_inp"] = pairs_inp
                    DNN_inputs["pairs_inp_sorted"] = pairs_inp_sorted
                    DNN_inputs["target"] = target
                else:
                    # check max number of jets of datasets and append EMPTY_FLOAT columns if necessary
                    if DNN_inputs["inputs"].shape[1] != events.shape[1]:
                        if DNN_inputs["inputs"].shape[1] > events.shape[1]:
                            n_extra_columns = DNN_inputs["inputs"].shape[1] - events.shape[1]
                            extra_columns = np.full((events.shape[0], n_extra_columns), EMPTY_FLOAT)
                            events = np.concatenate((events, extra_columns), axis=1)
                        else:
                            n_extra_columns = events.shape[1] - DNN_inputs["inputs"].shape[1]
                            extra_columns = np.full((DNN_inputs["inputs"].shape[0], n_extra_columns), EMPTY_FLOAT)
                            DNN_inputs["inputs"] = np.concatenate((DNN_inputs["inputs"], extra_columns), axis=1)
                    DNN_inputs["weights"] = np.concatenate([DNN_inputs["weights"], weights])
                    DNN_inputs["inputs"] = np.concatenate([DNN_inputs["inputs"], events])
                    DNN_inputs["inputs2"] = np.concatenate([DNN_inputs["inputs2"], events2])
                    DNN_inputs["pairs_inp"] = np.concatenate([DNN_inputs["pairs_inp"], pairs_inp], axis=0)
                    DNN_inputs["pairs_inp_sorted"] = np.concatenate([DNN_inputs["pairs_inp_sorted"], pairs_inp_sorted], axis=0)
                    DNN_inputs["target"] = np.concatenate([DNN_inputs["target"], target])
            logger.debug(f"   weights: {weights[:5]}")
            logger.debug(f"   Sum NN weights: {sum_nnweights}")

            logger.info(f"Inputs done for {dataset} in {(time.time() - t0):.3f}s")

        logger.info(f"Sum of weights per process: {sum_nnweights_processes}")

        # shuffle events and split into train and validation fold
        inputs_size = sum([arr.size * arr.itemsize for arr in DNN_inputs.values()])
        logger.info(f"inputs size is {inputs_size / 1024**3} GB")

        shuffle_indices = np.array(range(len(DNN_inputs["weights"])))
        np.random.shuffle(shuffle_indices)
        print('SHUFFLE:', shuffle_indices)

        validation_fraction = 0.25
        N_validation_events = int(validation_fraction * len(DNN_inputs["weights"]))
        train, validation = {}, {}
        for k in DNN_inputs.keys():
            DNN_inputs[k] = DNN_inputs[k][shuffle_indices]

            validation[k] = DNN_inputs[k][:N_validation_events]
            train[k] = DNN_inputs[k][N_validation_events:]
        # reshape and normalize inputs
        train, train_norm = reshape_norm_inputs(train, self.norm_features, self.input_features, self.n_output_nodes, self.baseline_jets, self.baseline_pairs, self.masking_val)
        validation, _ = reshape_norm_inputs(validation, self.norm_features, self.input_features, self.n_output_nodes, self.baseline_jets, self.baseline_pairs, self.masking_val)

        return train, train_norm, validation

    def instant_evaluate(
        self,
        task: law.Task,
        model,
        masking_model,
        feature_names,
        class_names,
        train: tf.data.Dataset,
        validation: tf.data.Dataset,
        output: law.LocalDirectoryTarget,
    ) -> None:
        # store the model history
        output.child("model_history.pkl", type="f").dump(model.history.history)
        output.child("model_history.pkl", type="f").dump(model.history.history)
        def call_func_safe(func, *args, **kwargs) -> Any:
            """
            Small helper to make sure that our training does not fail due to plotting
            """
            t0 = time.perf_counter()

            try:
                outp = func(*args, **kwargs)
            except Exception as e:
                print('Failed')
                outp = None

            return outp

        # evaluate training and validation sets
        if self.model_type == "baseline":
            train["prediction"] = call_func_safe(model, train['inputs_baseline'])
            validation["prediction"] = call_func_safe(model, validation['inputs_baseline'])
            call_func_safe(plot_shap_baseline, model, train, output, self.process_insts, self.target_dict, feature_names, self.features_pairs, self.baseline_jets, self.baseline_pairs, self.model_type)
        elif self.model_type == "baseline_pairs":
            train["prediction"] = call_func_safe(model, train['inputs_baseline_pairs'])
            validation["prediction"] = call_func_safe(model, validation['inputs_baseline_pairs'])
            # call_func_safe(plot_shap_baseline, model, train, output, self.process_insts, self.target_dict, feature_names, self.features_pairs, self.baseline_jets, self.baseline_pairs, self.model_type)
        elif self.model_type == "DeepSets":
            train["prediction"] = call_func_safe(model, [train['inputs'], train['inputs2']])
            validation["prediction"] = call_func_safe(model, [validation['inputs'], validation['inputs2']])
            # call_func_safe(plot_shap_deep_sets, model, train, output, self.process_insts, self.target_dict, feature_names, self.latex_dict)
            call_func_safe(PCA, model, train, self.model_type, output)
        elif self.model_type == "DeepSetsPS":
            train["prediction"] = call_func_safe(model, [[train['inputs'], train['pairs_inp']], train['inputs2']])
            validation["prediction"] = call_func_safe(model, [[validation['inputs'], validation['pairs_inp']], validation['inputs2']])
            # call_func_safe(plot_shap_deep_sets_ps, model, train, output, self.process_insts, self.target_dict, feature_names, self.features_pairs, self.latex_dict)
            call_func_safe(PCA, model, train, self.model_type, output)
        elif self.model_type == "DeepSetsPP":
            train["prediction"] = call_func_safe(model, [train["inputs"], train["pairs_inp"], train["inputs2"]])
            validation["prediction"] = call_func_safe(model, [validation["inputs"], validation["pairs_inp"], validation["inputs2"]])
            # call_func_safe(plot_shap_deep_sets_pp, model, train, output, self.process_insts, self.target_dict, feature_names, self.features_pairs, self.latex_dict)
            call_func_safe(PCA, model, train, self.model_type, output)

        # make some plots of the history
        call_func_safe(plot_accuracy, model.history.history, output)
        call_func_safe(plot_loss, model.history.history, output)

        """ insert the empty str "" such that plotting funcs are usable with and without
        the sorting argument used in first_nn. """
        # create some confusion matrices
        call_func_safe(plot_confusion, "", model, train, output, "train", self.process_insts)
        call_func_safe(plot_confusion, "", model, validation, output, "validation", self.process_insts)

        # create some ROC curves
        call_func_safe(plot_roc_ovr, "", train, output, "train", self.process_insts)
        call_func_safe(plot_roc_ovr, "", validation, output, "validation", self.process_insts)

        # create plots for all output nodes + Significance for each node
        call_func_safe(plot_output_nodes, "", model, train, validation, output, self.process_insts)
        call_func_safe(plot_significance, "", model, train, validation, output, self.process_insts)

        # plot some distributions of the train and validation to check that all folds are similar
        input1_train = [train['inputs'][:, :, 2].numpy().flatten(), feature_names[0][2]]
        input1_validation = [validation['inputs'][:, :, 2].numpy().flatten(), feature_names[0][2]]
        input2_train = [train['inputs'][:, :, 3].numpy().flatten(), feature_names[0][3]]
        input2_validation = [validation['inputs'][:, :, 3].numpy().flatten(), feature_names[0][3]]
        call_func_safe(check_distribution, output, input1_train[0], input1_train[1], self.masking_val, "train")
        call_func_safe(check_distribution, output, input1_validation[0], input1_validation[1], self.masking_val, "validation")
        call_func_safe(check_distribution, output, input2_train[0], input2_train[1], self.masking_val, "train")
        call_func_safe(check_distribution, output, input2_validation[0], input2_validation[1], self.masking_val, "validation")

    def train(
        self,
        task: law.Task,
        input: Any,
        output: law.LocalDirectoryTarget,
    ) -> ak.Array:
        # set seed for shuffeling for reproducebility
        np.random.seed(1337)
        tf.random.set_seed(1337)
        # Load Custom Model
        from hbt.ml.DNN_automated import CombinedDeepSetNetwork, BaseLineFF, ShapMasking
        from hbt.ml.DNN_pairs_parallel import CombinedDeepSetPairsParallelNetwork, BaseLineFFPairs
        from hbt.ml.DNN_pairs_sequential import CombinedDeepSetPairsSequentialNetwork
        physical_devices = tf.config.list_physical_devices("GPU")
        try:
            tf.config.experimental.set_memory_growth(physical_devices[0], True)
        except:
            # Invalid device or cannot modify virtual devices once initialized.
            pass

        train, train_norm, validation = self.prepare_inputs(task, input)

        gc.collect()
        logger.info("garbage collected")

        #
        # model preparations
        #

        # configureations for Deep Sets and FF Networt
        dict_vals = tf.stack(list(self.pairs_dict_tf.values()))
        deepset_config_jets = {'nodes': self.nodes_deepSets, 'activations': self.activation_func_deepSets,
            'n_l2': self.l2, 'aggregations': self.aggregations, 'masking_val': self.masking_val,
            'mean': train_norm['mean_jets'], 'std': train_norm['std_jets'], 'ff_mean': train_norm['mean_inputs2'],
            'ff_std': train_norm['std_inputs2'], 'event_to_jet': self.event_to_jet}
        deepset_config_pairs = {'nodes': self.nodes_deepSets, 'activations': self.activation_func_deepSets,
            'n_l2': self.l2, 'aggregations': self.aggregations, 'masking_val': self.masking_val,
            'mean': train_norm['mean_pairs'], 'std': train_norm['std_pairs'], 'ff_mean': train_norm['mean_inputs2'],
            'ff_std': train_norm['std_inputs2'], 'event_to_jet': self.event_to_jet}
        deepset_config_sequential = {'nodes': self.nodes_deepSets, 'activations': self.activation_func_deepSets,
            'n_l2': self.l2, 'aggregations': self.aggregations, 'masking_val': self.masking_val,
            'mean_jets': train_norm['mean_jets'], 'std_jets': train_norm['std_jets'],
            'mean_pairs': train_norm['mean_pairs'], 'std_pairs': train_norm['std_pairs'],
            'dict_vals': dict_vals, 'sequential_mode': self.sequential_mode}
        # FF config for the comb networks, 0 and 1 are padded in the NN to match the concat of DS OP and Inputs2
        feedforward_DS_config = {'nodes': self.nodes_ff, 'activations': self.activation_func_ff,
            'n_l2': self.l2, 'n_classes': self.n_output_nodes, 'mean': train_norm['mean_inputs2'],
            'std': train_norm['std_inputs2'], 'combined': True}
        # FF config for the baseline FF
        feedforward_config = {'nodes': self.nodes_ff, 'activations': self.activation_func_ff,
            'n_l2': self.l2, 'n_classes': self.n_output_nodes, 'mean': train_norm['mean_baseline'],
            'std': train_norm['std_baseline']}
        # FF config for the DS PP Network
        feedforward_pairs_config = {'nodes': self.nodes_ff, 'activations': self.activation_func_ff,
            'n_l2': self.l2, 'n_classes': self.n_output_nodes, 'mean': train_norm['mean_baseline_pairs'],
            'std': train_norm['std_baseline_pairs']}

        if self.model_type == "baseline":
            model = BaseLineFF(feedforward_config)
            tf_train = [train['inputs_baseline'], train['target']]
            tf_validation = [validation['inputs_baseline'], validation['target']]
        elif self.model_type == "baseline_pairs":
            model = BaseLineFFPairs(feedforward_pairs_config)
            tf_train = [train['inputs_baseline_pairs'], train['target']]
            tf_validation = [validation['inputs_baseline_pairs'], validation['target']]
        elif self.model_type == "DeepSets":
            model = CombinedDeepSetNetwork(deepset_config_jets, feedforward_DS_config)
            tf_train = [[train['inputs'], train['inputs2']], train['target']]
            tf_validation = [[validation['inputs'], validation['inputs2']], validation['target']]
        elif self.model_type == "DeepSetsPS":
            model = CombinedDeepSetPairsSequentialNetwork(deepset_config_sequential, feedforward_DS_config)
            tf_train = [[[train['inputs'], train['pairs_inp']], train['inputs2']], train['target']]
            tf_validation = [[[validation['inputs'], validation['pairs_inp']], validation['inputs2']], validation['target']]
        elif self.model_type == "DeepSetsPP":
            deepset_config_jets["inp_type"] = "Jets"
            deepset_config_pairs["inp_type"] = "Pairs"
            model = CombinedDeepSetPairsParallelNetwork(deepset_config_jets, deepset_config_pairs, feedforward_DS_config)
            tf_train = [[train['inputs'], train['pairs_inp'], train['inputs2']], train['target']]
            tf_validation = [[validation['inputs'], validation['pairs_inp'], validation['inputs2']], validation['target']]

        optimizer = keras.optimizers.Adam(
            learning_rate=self.learningrate, beta_1=0.9, beta_2=0.999,
            epsilon=1e-6, amsgrad=False,
        )
        model.compile(
            loss="categorical_crossentropy",
            optimizer=optimizer,
            weighted_metrics=["categorical_accuracy"],
            run_eagerly=False,
        )

        #
        # training
        #

        # early stopping to determine the 'best' model
        early_stopping = tf.keras.callbacks.EarlyStopping(
            monitor="val_loss",
            min_delta=0,
            patience=int(self.epochs / 4),
            verbose=1,
            mode="auto",
            baseline=None,
            restore_best_weights=True,
            start_from_epoch=0,
        )

        reduceLR = tf.keras.callbacks.ReduceLROnPlateau(
            monitor="val_loss",
            factor=0.1,
            patience=int(self.epochs / 6),
            verbose=1,
            mode="auto",
            min_lr=0.01 * self.learningrate,
        )

        logger.info("input to tf Dataset")

        # transform the the keys of self.ml_process_weights to their respective idx
        # so that the dict is understood by the model
        class_weights_dict = {}
        for proc_name, target in self.target_dict.items():
            class_weights_dict[target] = self.ml_process_weights[proc_name]
        fit_kwargs = {
            "epochs": self.epochs,
            "callbacks": [early_stopping, reduceLR],
            "verbose": 2,
            "class_weight": class_weights_dict,
        }

        # train the model
        logger.info(f"Loss training weights: {self.ml_process_weights.items()}")
        logger.info("Start training...")

        model.fit(
            tf_train[0], tf_train[1],
            validation_data=tf_validation,
            batch_size=self.batchsize,
            **fit_kwargs,
        )

        # save the model and history; TODO: use formatter
        # output.dump(model, formatter="tf_keras_model")
        output.parent.touch()
        model.save(output.path)

        # plotting of loss, acc, roc, nodes, confusion, shap plots for each k-fold
        # pass the same model twice, the second is used as a filler since two different models need
        # to be passed for Deep Sets (SHAP Masking)
        if self.model_type == "baseline" or self.model_type == "baseline_pairs":
            self.instant_evaluate(task, model, model, self.input_features, self.processes, train, validation, output)
        elif self.model_type == "DeepSets" or self.model_type == "DeepSetsPS":
            # slice_idx = tf_train[0][0].shape[2]
            slice_idx = None
            masking_model = ShapMasking(slice_idx, model)
            self.instant_evaluate(task, model, masking_model, self.input_features, self.processes, train, validation, output)
        elif self.model_type == "DeepSetsPP":
            self.instant_evaluate(task, model, model, self.input_features, self.processes, train, validation, output)
        write_info_file(output, self.aggregations, self.nodes_deepSets, self.nodes_ff,
            self.n_output_nodes, self.batch_norm_deepSets, self.batch_norm_ff, self.input_features,
            self.process_insts, self.activation_func_deepSets, self.activation_func_ff, self.learningrate,
            self.ml_process_weights, self.jet_num_cut, self.ml_process_weights,
            self.model_type, self.jet_collection, self.projection_phi, self.sequential_mode, self.l2,
            self.event_to_jet)

    def evaluate(
        self,
        task: law.Task,
        events: ak.Array,
        models: list(Any),
        fold_indices: ak.Array,
        events_used_in_training: bool = True,
    ) -> None:

        # output = task.target(f"mlmodel_f{task.branch}of{self.folds}_{self.model_name}", dir=True)
        # output_all_folds = task.target(f"mlmodel_all_folds_{self.model_name}", dir=True)
        logger.info(f"Evaluation of dataset {task.dataset}")
        proc_name = "_".join(task.dataset.split("_")[:-1])
        proc_name = "tt" if "tt" in proc_name and "tt" in self.processes else proc_name
        proc_name = "dy" if "dy" in proc_name and "dy" in self.processes else proc_name
        models, history = zip(*models)
        model0 = models[0]
        if self.model_type == "DeepSetsPS":
            max_jets = model0.signatures["serving_default"].structured_input_signature[1]['input_1_1'].shape[1]
        else:
            max_jets = model0.signatures["serving_default"].structured_input_signature[1]['input_1'].shape[1]
        # create a copy of the inputs to use for evaluation
        inputs = ak.copy(events)
        events2 = events[self.input_features[1]]
        events1 = events[self.input_features[0]]
        events_pairs = events[self.pair_vectors]
        projection_phi = events[self.projection_phi[0]]

        # prepare the input features
        events1 = reshape_raw_inputs1(events1, self.n_features, self.input_features[0], projection_phi)
        events2 = reshape_raw_inputs2(events2)
        events_pairs = reshape_raw_inputs1(events_pairs, len(self.pair_vectors), self.pair_vectors, projection_phi)
        pairs_inp, pairs_sorted, _ = create_pairs(events_pairs, self.pairs_dict_pad, self.pair_vectors, self.masking_val, 10)

        # add extra colums to the deep sets input to fir the required input shape of the model
        # padding of extra columns must use the EMPTY_FLOAT, will be replaced with masking value
        # in reshape_norm_inputs
        n_columns = max_jets * len(self.input_features[0])
        n_extra_columns = n_columns - events1.shape[1]
        if n_extra_columns != 0:
            extra_columns = np.full((events1.shape[0], n_extra_columns), EMPTY_FLOAT)
            events1 = np.concatenate((events1, extra_columns), axis=1)

        # create target and add to test dict
        target_idx = self.processes.index(proc_name)
        target = np.zeros((events1.shape[0], len(self.processes)))
        target[:, target_idx] = 1

        test = {'inputs': events1,
                'inputs2': events2,
                'pairs_inp': pairs_inp,
                'pairs_inp_sorted': pairs_sorted,
                'target': target,
                }

        test, _ = reshape_norm_inputs(test, self.norm_features, self.input_features, self.n_output_nodes, self.baseline_jets, self.baseline_pairs, self.masking_val)
        # inputs to feed to the model
        if self.model_type == "baseline":
            inputs = test['inputs_baseline']
        elif self.model_type == "baseline_pairs":
            inputs = test['inputs_baseline_pairs']
        elif self.model_type == "DeepSetsPP":
            inputs = [test["inputs"], test["pairs_inp"], test["inputs2"]]
        elif self.model_type == "DeepSetsPS":
            inputs = [[test["inputs"], test["pairs_inp"]], test["inputs2"]]
        elif self.model_type == "DeepSets":
            inputs = [test["inputs"], test["inputs2"]]

        # do prediction for all models and all inputs
        predictions = []
        for i, model in enumerate(models):
            pred = model.predict(inputs)
            pred = ak.from_numpy(pred)
            if len(pred[0]) != len(self.processes):
                raise Exception("Number of output nodes should be equal to number of processes")
            predictions.append(pred)

        '''In pred, each model sees the complete set of data, this includes data used for training
        and validation. For each model, keep only the predictions on inputs that were not yet seen
        by the model during training/validation. Keep only prediction on the subset k that was not
        by the model. Combine all of the predictions on the k subsets by the k different
        models into one prediction array'''
        '''outputs: generate array with shape of the final pred array with only entries -1
        -> later overriden. Since all k substes combined are the complete set of data, all
        entries in outputs will later be overriden with prediction by the model associated with
        a given subset.'''
        # combine all models into 1 output score, using the model that has not seen test set yet
        outputs = ak.where(ak.ones_like(predictions[0]), -1, -1)
        op = ak.where(ak.ones_like(predictions[0]), -1, -1)
        weights = np.full(len(outputs), 0)
        target_val = np.full(len(outputs), target_idx)
        events = set_ak_column(events, f"{self.cls_name}.ml_truth_label_{proc_name}__{self.model_name}", target_val)
        for i in range(self.folds):
            logger.info(f"Evaluation fold {i}")
            # output = task.target(f"mlmodel_f{task.branch}of{self.folds}_{self.model_name}", dir=True)
            # output = task.target(f"mlmodel_f{i}of{self.folds}_{self.model_name}", dir=True)
            # reshape mask from N*bool to N*k*bool (TODO: simpler way?)
            '''get indices of the events that belong to k subset not yet seen by the model and
            override the entries at these indices in outputs with the prediction of the model.'''
            idx = ak.to_regular(ak.concatenate([ak.singletons(fold_indices == i)] * len(self.processes), axis=1))
            outputs = ak.where(idx, predictions[i], outputs)
            fold_op = ak.where(idx, predictions[i], op)
            fold_op = np.concatenate((fold_op, target), axis=1)

            mask_weights = (np.sum(idx, axis=1) != 0)
            weights = ak.where(mask_weights, events.normalization_weight, weights)
            w = np.array(weights).reshape(-1, 1)
            fold_op = np.concatenate((fold_op, w), axis=1)
            events = set_ak_column(events, f"{self.cls_name}.pred_fold_{i}_{proc_name}__{self.model_name}", fold_op)
            # get the inp of the Deep Sets for each test fold
            # filler = np.full_like(inputs[0], self.masking_val)
            # padded_fold_inp = np.concatenate((inputs[0].numpy()[mask_weights], filler[~mask_weights]), axis=0)
            # padded_fold_inp_pt = np.reshape(padded_fold_inp[:, :, 2], (padded_fold_inp.shape[0], -1))[:, :9]
            # events = set_ak_column(events, f"{self.cls_name}.DeepSetsInpPt_{proc_name}_fold{i}__{self.model_name}", ak.Array(padded_fold_inp_pt))
            # padded_fold_inp_eta = np.reshape(padded_fold_inp[:, :, 3], (padded_fold_inp.shape[0], -1))[:, :9]
            # events = set_ak_column(events, f"{self.cls_name}.DeepSetsInpEta_{proc_name}_fold{i}__{self.model_name}", ak.Array(padded_fold_inp_eta))

        test['prediction'] = np.squeeze(outputs)

        if len(outputs[0]) != len(self.processes):
            raise Exception("Number of output nodes should be equal to number of processes")

        for i, proc in enumerate(self.processes):
            events = set_ak_column(events, f"mlscore.{proc}", outputs[:, i])

        events = set_ak_column(events, f"{self.cls_name}.predictions_{proc_name}__{self.model_name}", test["prediction"])
        pred_target = np.concatenate((test["prediction"], target), axis=1)
        events = set_ak_column(events, f"{self.cls_name}.pred_target_{proc_name}__{self.model_name}", pred_target)
        events = set_ak_column(events, f"{self.cls_name}.target_label_{proc_name}__{self.model_name}", np.full(target.shape[0], target_idx))
        events = set_ak_column(events, f"{self.cls_name}.events_weights_{proc_name}__{self.model_name}", weights)

        return events
